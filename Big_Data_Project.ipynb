{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRxDOCumz0HV",
        "outputId": "d9ead4a1-2753-42be-8474-511891d8106c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pmdarima in /usr/local/lib/python3.10/dist-packages (2.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.4.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.14.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.0.7)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (67.7.2)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.13.2->pmdarima) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!pip install pmdarima"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JJCYiMqOpVV0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJzh_YtC5Hsy",
        "outputId": "c7be742e-a30c-4f3b-bf8e-4283114fdcbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ],
      "source": [
        "# Install Kaggle API\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "z8nwNm-Tz3hS",
        "outputId": "f1304cc5-989d-432f-ab1d-c59ed4999f9d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3c009807-c09a-4c53-b3d9-3e80817115bc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3c009807-c09a-4c53-b3d9-3e80817115bc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n"
          ]
        }
      ],
      "source": [
        "# Upload kaggle.json\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # This will prompt you to upload the kaggle.json file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_4hhXZqrz3j7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Load the JSON file\n",
        "with open('kaggle.json', 'r') as f:\n",
        "    kaggle_creds = json.load(f)\n",
        "    os.environ['KAGGLE_USERNAME'] = kaggle_creds['username']\n",
        "    os.environ['KAGGLE_KEY'] = kaggle_creds['key']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIqjgJYhz3ox",
        "outputId": "42c18368-7cd0-4c89-fd5b-9eb0f4068ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/anlgrbz/student-demographics-online-education-dataoulad\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "student-demographics-online-education-dataoulad.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  student-demographics-online-education-dataoulad.zip\n",
            "replace assessments.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d anlgrbz/student-demographics-online-education-dataoulad\n",
        "\n",
        "! unzip \"student-demographics-online-education-dataoulad.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMJtnwQDz3qT",
        "outputId": "22783c92-b834-48fa-d644-d4aab1ef1876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.metrics import mean_squared_error , mean_absolute_error, r2_score\n",
        "from pmdarima.arima.utils import nsdiffs\n",
        "from pmdarima import auto_arima\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from xgboost import XGBRegressor\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title='Open University Learning Analytics Dataset (OULAD) Analysis',\n",
        "    page_icon='ðŸ“Š'\n",
        ")\n",
        "\n",
        "# -------------------------------------Functions--------------------------------------------\n",
        "# ----------------------------------- -Defination--------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    assesment = pd.read_csv('/content/assessments.csv')\n",
        "    course = pd.read_csv('/content/courses.csv')\n",
        "    as_stu = pd.read_csv('/content/studentAssessment.csv')\n",
        "    info_stu = pd.read_csv('/content/studentInfo.csv')\n",
        "    reg_stu = pd.read_csv('/content/studentRegistration.csv')\n",
        "    vle_stu = pd.read_csv('/content/studentVle.csv')\n",
        "    vle = pd.read_csv('/content/vle.csv')\n",
        "    return assesment, course, as_stu, info_stu, reg_stu, vle_stu, vle\n",
        "\n",
        "assesment, course, as_stu, info_stu, reg_stu, vle_stu, vle = load_data()\n",
        "\n",
        "def missingValueAssessment(data):\n",
        "    # Display data information\n",
        "    st.title(\"Missing Value Assessment\")\n",
        "\n",
        "    st.write(\"## Data Information\")\n",
        "    buffer = io.StringIO()\n",
        "    data.info(buf=buffer)\n",
        "    s = buffer.getvalue()\n",
        "    st.text(s)\n",
        "\n",
        "    st.write(\"### Dataframe Shape\")\n",
        "    st.write(f\"**Rows**: {data.shape[0]}, **Columns**: {data.shape[1]}\")\n",
        "\n",
        "    st.write(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "    # Calculate missing values\n",
        "    missing_values = data.isnull().sum()\n",
        "    missing_values_percentage = (missing_values / len(data)) * 100\n",
        "\n",
        "    # DataFrame to display missing values\n",
        "    missing_values_df = pd.DataFrame({\n",
        "        'Column': missing_values.index,\n",
        "        'Missing Values': missing_values.values,\n",
        "        'Percentage': missing_values_percentage\n",
        "    }).sort_values(by='Missing Values', ascending=False)\n",
        "\n",
        "    # Display missing values in a table\n",
        "    st.write(\"### Missing Values\")\n",
        "    st.dataframe(missing_values_df)\n",
        "\n",
        "    # Plot missing values\n",
        "    st.write(\"### Missing Values Bar Chart\")\n",
        "    fig = px.bar(missing_values_df, x='Column', y='Missing Values',\n",
        "                 title='Missing Values by Column',\n",
        "                 labels={'Missing Values': 'Count of Missing Values'},\n",
        "                 color='Missing Values',\n",
        "                 color_continuous_scale=px.colors.sequential.Viridis)\n",
        "\n",
        "    fig.update_layout(xaxis_tickangle=-45)\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # Display missing values percentage\n",
        "    st.write(\"### Missing Values Percentage\")\n",
        "    st.dataframe(missing_values_df[['Column', 'Percentage']])\n",
        "\n",
        "    # Plot missing values percentage\n",
        "    st.write(\"### Missing Values Percentage Bar Chart\")\n",
        "    fig_percentage = px.bar(missing_values_df, x='Column', y='Percentage',\n",
        "                            title='Percentage of Missing Values by Column',\n",
        "                            labels={'Percentage': 'Percentage of Missing Values'},\n",
        "                            color='Percentage',\n",
        "                            color_continuous_scale=px.colors.sequential.Viridis)\n",
        "\n",
        "    fig_percentage.update_layout(xaxis_tickangle=-45)\n",
        "    st.plotly_chart(fig_percentage)\n",
        "\n",
        "    st.write(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    # Display the missing values\n",
        "    st.write(\"### Missing Values DataFrame\")\n",
        "    st.write(missing_values_df)\n",
        "\n",
        "    st.write(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "    # Visualize the missing values\n",
        "    st.write(\"### Missing Values Visualization\")\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Percentage', y='Column', data=missing_values_df.sort_values(by='Percentage', ascending=False))\n",
        "    plt.title('Percentage of Missing Values by Column')\n",
        "    plt.xlabel('Percentage of Missing Values')\n",
        "    plt.ylabel('Columns')\n",
        "    st.pyplot(plt)\n",
        "\n",
        "#Exploratory data analysis\n",
        "def plotActivityCounts(data):\n",
        "    #1. Compute the activity counts\n",
        "    st.subheader('Count of Each Activity Type')\n",
        "    activity_counts = data['activity_type'].value_counts().reset_index()\n",
        "    activity_counts.columns = ['activity_type', 'count']\n",
        "\n",
        "    # Sort the activity types by count in descending order\n",
        "    activity_counts = activity_counts.sort_values(by='count', ascending=False)\n",
        "\n",
        "    # Define a custom color sequence\n",
        "    custom_colors = px.colors.qualitative.Pastel\n",
        "\n",
        "    # bar chart with Plotly\n",
        "    fig_activity = px.bar(activity_counts,\n",
        "                          x='activity_type',\n",
        "                          y='count',\n",
        "                          title='Count of Each Activity Type',\n",
        "                          labels={'activity_type': 'Activity Type', 'count': 'Count'},\n",
        "                          text='count',\n",
        "                          color='activity_type',\n",
        "                          color_discrete_sequence=custom_colors)\n",
        "\n",
        "    # Update layout for better appearance\n",
        "    fig_activity.update_layout(\n",
        "        xaxis_title='Activity Type',\n",
        "        yaxis_title='Count',\n",
        "        title={'x': 0.5, 'xanchor': 'center'},\n",
        "        template='plotly_white',\n",
        "        xaxis_tickangle=45  # Rotate x-axis labels for better readability\n",
        "    )\n",
        "\n",
        "    # Display the Plotly chart in Streamlit\n",
        "    st.plotly_chart(fig_activity)\n",
        "\n",
        "    #2. Counts of code_presentation for each code_module\n",
        "    st.subheader('Counts of Code Presentations for Each Code Module')\n",
        "    df_counts_module_presentation = data.groupby(['code_module', 'code_presentation']).size().reset_index(name='count')\n",
        "\n",
        "    # stacked bar chart with Plotly\n",
        "    fig_module_presentation = px.bar(df_counts_module_presentation,\n",
        "                                     x='code_module',\n",
        "                                     y='count',\n",
        "                                     color='code_presentation',\n",
        "                                     title='Counts of Code Presentations for Each Code Module',\n",
        "                                     labels={'code_module': 'Code Module', 'count': 'Count', 'code_presentation': 'Code Presentation'},\n",
        "                                     color_discrete_sequence=px.colors.sequential.Viridis,\n",
        "                                     barmode='stack')\n",
        "\n",
        "    # Update layout for better appearance\n",
        "    fig_module_presentation.update_layout(\n",
        "        xaxis_title='Code Module',\n",
        "        yaxis_title='Count',\n",
        "        title={'x': 0.5, 'xanchor': 'center'},\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    # Display the Plotly chart in Streamlit\n",
        "    st.plotly_chart(fig_module_presentation)\n",
        "\n",
        "    #3. Count of code_presentation\n",
        "    st.subheader('Count of Code Presentations')\n",
        "    df_counts_presentation = data['code_presentation'].value_counts().reset_index()\n",
        "    df_counts_presentation.columns = ['code_presentation', 'count']\n",
        "\n",
        "    # bar chart with Plotly\n",
        "    fig_presentation = px.bar(df_counts_presentation,\n",
        "                              x='count',\n",
        "                              y='code_presentation',\n",
        "                              title='Count of Code Presentations',\n",
        "                              labels={'code_presentation': 'Code Presentation', 'count': 'Count'},\n",
        "                              text='count',\n",
        "                              color='code_presentation',\n",
        "                              color_discrete_sequence=custom_colors,\n",
        "                              orientation='h')\n",
        "\n",
        "    # Update layout for better appearance\n",
        "    fig_presentation.update_layout(\n",
        "        xaxis_title='Count',\n",
        "        yaxis_title='Code Presentation',\n",
        "        title={'x': 0.5, 'xanchor': 'center'},\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    # Display the Plotly chart in Streamlit\n",
        "    st.plotly_chart(fig_presentation)\n",
        "\n",
        "    #4. Count of code_module\n",
        "    st.subheader('Count of Code Modules')\n",
        "    df_counts_module = data['code_module'].value_counts().reset_index()\n",
        "    df_counts_module.columns = ['code_module', 'count']\n",
        "\n",
        "    # bar chart with Plotly\n",
        "    fig_module = px.bar(df_counts_module,\n",
        "                        x='count',\n",
        "                        y='code_module',\n",
        "                        title='Count of Code Modules',\n",
        "                        labels={'code_module': 'Code Module', 'count': 'Count'},\n",
        "                        text='count',\n",
        "                        color='code_module',\n",
        "                        color_discrete_sequence=custom_colors,\n",
        "                        orientation='h')\n",
        "\n",
        "    # Update layout for better appearance\n",
        "    fig_module.update_layout(\n",
        "        xaxis_title='Count',\n",
        "        yaxis_title='Code Module',\n",
        "        title={'x': 0.5, 'xanchor': 'center'},\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    # Display the Plotly chart in Streamlit\n",
        "    st.plotly_chart(fig_module)\n",
        "def student_vle_eda(data):\n",
        "    # Calculate daily interactions sum\n",
        "    daily_interactions = data.groupby('date')['sum_click'].sum().reset_index()\n",
        "\n",
        "    # Plotting with Plotly Express\n",
        "    st.write(\"# Time Series Plot of Daily Student Interactions with VLE\")\n",
        "\n",
        "    fig = px.line(daily_interactions, x='date', y='sum_click',\n",
        "                  title='Daily Student Interactions with VLE',\n",
        "                  labels={'date': 'Date', 'sum_click': 'Sum of Clicks'})\n",
        "\n",
        "    # Update layout for better appearance\n",
        "    fig.update_layout(\n",
        "        xaxis_title='Date',\n",
        "        yaxis_title='Sum of Clicks',\n",
        "        title={'x': 0.5, 'xanchor': 'center'},\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    # Display the Plotly chart in Streamlit\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "\n",
        "def reg_course_eda(data):\n",
        "    #1 .Count of Code Presentations\n",
        "    st.subheader('Count of Code Presentations')\n",
        "\n",
        "    # Group by 'code_presentation' and count occurrences\n",
        "    df_counts_presentation = data['code_presentation'].value_counts().reset_index()\n",
        "    df_counts_presentation.columns = ['code_presentation', 'count']\n",
        "\n",
        "    # bar chart with Plotly\n",
        "    fig_presentation = px.bar(df_counts_presentation,\n",
        "                              x='count',\n",
        "                              y='code_presentation',\n",
        "                              title='Count of Code Presentations',\n",
        "                              labels={'code_presentation': 'Code Presentation', 'count': 'Count'},\n",
        "                              color='code_presentation',  # Set color by code_presentation\n",
        "                              color_discrete_sequence=px.colors.qualitative.Dark2)\n",
        "\n",
        "    # Update layout for better appearance\n",
        "    fig_presentation.update_layout(\n",
        "        xaxis_title='Count',\n",
        "        yaxis_title='Code Presentation',\n",
        "        title={'x': 0.5, 'xanchor': 'center'},\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    # Display the Plotly chart in Streamlit\n",
        "    st.plotly_chart(fig_presentation)\n",
        "\n",
        "    #2 .Count of Code Modules\n",
        "    st.subheader('Count of Code Modules')\n",
        "\n",
        "    df_counts_module = data['code_module'].value_counts().reset_index()\n",
        "    df_counts_module.columns = ['code_module', 'count']\n",
        "\n",
        "    # bar chart with Plotly\n",
        "    fig_module = px.bar(df_counts_module,\n",
        "                        x='count',\n",
        "                        y='code_module',\n",
        "                        title='Count of Code Modules',\n",
        "                        labels={'code_module': 'Code Module', 'count': 'Count'},\n",
        "                        color='code_module',  # Set color by code_module\n",
        "                        color_discrete_sequence=px.colors.qualitative.Dark2,\n",
        "                        orientation='h')  # Horizontal bar chart\n",
        "\n",
        "    # Update layout for better appearance\n",
        "    fig_module.update_layout(\n",
        "        xaxis_title='Count',\n",
        "        yaxis_title='Code Module',\n",
        "        title={'x': 0.5, 'xanchor': 'center'},\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    # Display the Plotly chart in Streamlit\n",
        "    st.plotly_chart(fig_module)\n",
        "def stu_reg_eda(data):\n",
        "    # Display the header for the data\n",
        "    st.subheader('Registration Count of Students With Time')\n",
        "\n",
        "    # 1. Aggregate interactions per day\n",
        "    daily_interactions = data.groupby('date_registration')['id_student'].nunique().reset_index()\n",
        "\n",
        "    # Display the first few rows\n",
        "    st.dataframe(daily_interactions.head())\n",
        "\n",
        "    #time series plot with Plotly\n",
        "    fig = px.line(\n",
        "        daily_interactions,\n",
        "        x='date_registration',\n",
        "        y='id_student',\n",
        "        title='Count of Student Registration with Time',\n",
        "        labels={'date_registration': 'Date of Registration', 'id_student': 'Count of Students'},\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    # Update layout for better appearance\n",
        "    fig.update_layout(\n",
        "        xaxis_title='Date of Registration',\n",
        "        yaxis_title='Count of Students',\n",
        "        title={'x': 0.5, 'xanchor': 'center'},\n",
        "        xaxis_tickangle=45\n",
        "    )\n",
        "\n",
        "    # 2. Display the Plotly chart in Streamlit\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    #--------------------------------------------------------------------------------------------------------------\n",
        "    st.subheader('Registration Count Per Module of Students With Time')\n",
        "\n",
        "    # Aggregate the registration counts per module over time\n",
        "    registration_trends = data.groupby(['date_registration', 'code_module']).size().reset_index(name='count')\n",
        "\n",
        "    #line plot with Plotly\n",
        "    fig = px.line(\n",
        "        registration_trends,\n",
        "        x='date_registration',\n",
        "        y='count',\n",
        "        color='code_module',\n",
        "        title='Student Registration Trends for Each Module',\n",
        "        labels={'date_registration': 'Date of Registration', 'count': 'Count of Registrations'},\n",
        "        markers=True,\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    # Update layout for better appearance\n",
        "    fig.update_layout(\n",
        "        xaxis_title='Date of Registration',\n",
        "        yaxis_title='Count of Registrations',\n",
        "        title={'x': 0.5, 'xanchor': 'center'},\n",
        "        xaxis_tickangle=45\n",
        "    )\n",
        "\n",
        "    # Display the Plotly chart in Streamlit\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "def info_stu_eda(info_stu):\n",
        "    st.subheader('Head of the data frame')\n",
        "    st.write(info_stu.head(10))\n",
        "    # 1. Count of Gender\n",
        "    st.subheader('Count of Gender')\n",
        "    gender_counts = info_stu['gender'].value_counts().reset_index()\n",
        "    gender_counts.columns = ['gender', 'count']\n",
        "    custom_colors = px.colors.qualitative.Pastel\n",
        "\n",
        "    fig = px.bar(gender_counts, x='count', y='gender',\n",
        "                 title='Count of Gender',\n",
        "                 labels={'gender': 'Gender', 'count': 'Count'},\n",
        "                 text='count',\n",
        "                 color='gender',\n",
        "                 color_discrete_sequence=custom_colors)\n",
        "\n",
        "    fig.update_layout(xaxis_title='Count', yaxis_title='Gender', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 2. Distribution of Gender\n",
        "    st.subheader('Distribution of Gender')\n",
        "    gender_counts = info_stu['gender'].value_counts().reset_index()\n",
        "    gender_counts.columns = ['gender', 'count']\n",
        "\n",
        "    fig = px.pie(gender_counts, names='gender', values='count',\n",
        "                 title='Distribution of Gender',\n",
        "                 color='gender',\n",
        "                 color_discrete_sequence=px.colors.qualitative.Pastel)\n",
        "\n",
        "    fig.update_layout(template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 3. Count of Students by Age Band\n",
        "    st.subheader('Count of Students by Age Band')\n",
        "    age_band_counts = info_stu['age_band'].value_counts().reset_index()\n",
        "    age_band_counts.columns = ['age_band', 'count']\n",
        "    age_band_counts = age_band_counts.sort_values(by='age_band')\n",
        "\n",
        "    fig = px.bar(age_band_counts, x='age_band', y='count',\n",
        "                 title='Count of Students by Age Band',\n",
        "                 labels={'age_band': 'Age Band', 'count': 'Count'},\n",
        "                 text='count',\n",
        "                 color='age_band',\n",
        "                 color_discrete_sequence=px.colors.qualitative.Pastel)\n",
        "\n",
        "    fig.update_layout(xaxis_title='Age Band', yaxis_title='Count', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 4. Count of Students by Region\n",
        "    st.subheader('Count of Students by Region')\n",
        "    region_counts = info_stu['region'].value_counts().reset_index()\n",
        "    region_counts.columns = ['region', 'count']\n",
        "    region_counts = region_counts.sort_values(by='region')\n",
        "\n",
        "    fig = px.bar(region_counts, x='region', y='count',\n",
        "                 title='Count of Students by Region',\n",
        "                 labels={'region': 'Region', 'count': 'Count'},\n",
        "                 text='count',\n",
        "                 color='region',\n",
        "                 color_discrete_sequence=px.colors.qualitative.Pastel)\n",
        "\n",
        "    fig.update_layout(xaxis_title='Region', yaxis_title='Count', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 5. Stacked Bar Chart of Region by Age Band\n",
        "    st.subheader('Stacked Bar Chart of Region by Age Band')\n",
        "    cross_tab = pd.crosstab(info_stu['region'], info_stu['age_band'])\n",
        "    age_bands = cross_tab.columns\n",
        "\n",
        "    fig = go.Figure()\n",
        "    for age_band in age_bands:\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=cross_tab.index,\n",
        "            y=cross_tab[age_band],\n",
        "            name=f'Age Band {age_band}',\n",
        "            marker_color=px.colors.qualitative.Pastel[age_bands.get_loc(age_band)],\n",
        "            hovertemplate='%{y}',\n",
        "            text=cross_tab[age_band],\n",
        "            textposition='auto'\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(barmode='stack', xaxis_title='Region', yaxis_title='Count',\n",
        "                      title='Stacked Bar Chart of Region by Age Band',\n",
        "                      legend_title='Age Band', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 6. Box Plot of Studied Credits by Region\n",
        "    st.subheader('Box Plot of Studied Credits by Region ')\n",
        "    fig = px.box(info_stu, x='studied_credits', y='region', color='region',\n",
        "                 orientation='h', title='Boxplot of Studied Credits by Region',\n",
        "                 labels={'region': 'Region', 'studied_credits': 'Studied Credits'},\n",
        "                 category_orders={'region': sorted(info_stu['region'].unique())})\n",
        "\n",
        "    fig.update_layout(xaxis_title='Studied Credits', yaxis_title='Region', template='plotly_white')\n",
        "    fig.update_traces(whiskerwidth=0.5)\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 7.Count Plot of Highest Education\n",
        "    st.subheader('Count Plot of Highest Education')\n",
        "    education_counts = info_stu['highest_education'].value_counts().reset_index()\n",
        "    education_counts.columns = ['highest_education', 'count']\n",
        "    education_counts = education_counts.sort_values(by='count', ascending=False)\n",
        "\n",
        "    fig = px.bar(education_counts, x='count', y='highest_education',\n",
        "                 title='Count of Highest Education',\n",
        "                 labels={'highest_education': 'Highest Education', 'count': 'Count'},\n",
        "                 orientation='h',\n",
        "                 color='highest_education',\n",
        "                 color_discrete_sequence=px.colors.qualitative.Dark2)\n",
        "\n",
        "    fig.update_layout(xaxis_title='Count', yaxis_title='Highest Education', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 8. Count Of Exam Results\n",
        "    st.subheader('Count Of Exam Results')\n",
        "    activity_counts = info_stu['final_result'].value_counts().reset_index()\n",
        "    activity_counts.columns = ['final_result', 'count']\n",
        "    activity_counts = activity_counts.sort_values(by='count', ascending=False)\n",
        "\n",
        "    fig = px.bar(activity_counts, x='final_result', y='count',\n",
        "                 title='Count of Exam Results',\n",
        "                 labels={'final_result': 'Result Status', 'count': 'Count'},\n",
        "                 color='final_result',\n",
        "                 color_discrete_sequence=px.colors.qualitative.Set2)\n",
        "\n",
        "    fig.update_layout(xaxis_title='Result Status', yaxis_title='Count', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 9. Final Results by Gender\n",
        "    st.subheader('Final Results by Gender')\n",
        "    final_result_gender_counts = info_stu.groupby(['final_result', 'gender']).size().reset_index(name='count')\n",
        "    sorted_results = final_result_gender_counts.groupby('final_result').sum().sort_values(by='count', ascending=False).index\n",
        "\n",
        "    fig = px.bar(final_result_gender_counts, x='final_result', y='count', color='gender',\n",
        "                 title='Final Results by Gender (Sorted)',\n",
        "                 labels={'final_result': 'Final Result', 'count': 'Count'},\n",
        "                 category_orders={'final_result': sorted_results},\n",
        "                 color_discrete_sequence=px.colors.qualitative.Set2)\n",
        "\n",
        "    fig.update_layout(xaxis_title='Final Result', yaxis_title='Count', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 10. Final Results by Region\n",
        "    st.subheader('Final Results by Region')\n",
        "    final_result_region_counts = info_stu.groupby(['final_result', 'region']).size().reset_index(name='count')\n",
        "    sorted_results = final_result_region_counts.groupby('final_result').sum().sort_values(by='count', ascending=False).index\n",
        "    pivot_data = final_result_region_counts.pivot(index='final_result', columns='region', values='count').reindex(sorted_results).fillna(0)\n",
        "\n",
        "    fig = go.Figure()\n",
        "    for region in pivot_data.columns:\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=pivot_data.index,\n",
        "            y=pivot_data[region],\n",
        "            name=region,\n",
        "            hovertemplate='%{y}',\n",
        "            text=pivot_data[region],\n",
        "            textposition='auto'\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(barmode='stack', xaxis_title='Final Result', yaxis_title='Count',\n",
        "                      title='Final Results by Region (Stacked)', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 11. Final Results by Highest Education\n",
        "    st.subheader('Final Results by Highest Education')\n",
        "    final_result_education_counts = info_stu.groupby(['final_result', 'highest_education']).size().reset_index(name='count')\n",
        "    sorted_results = final_result_education_counts.groupby('final_result').sum().sort_values(by='count', ascending=False).index\n",
        "    pivot_data = final_result_education_counts.pivot(index='final_result', columns='highest_education', values='count').reindex(sorted_results).fillna(0)\n",
        "\n",
        "    fig = go.Figure()\n",
        "    for education in pivot_data.columns:\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=pivot_data.index,\n",
        "            y=pivot_data[education],\n",
        "            name=education,\n",
        "            hovertemplate='%{y}',\n",
        "            text=pivot_data[education],\n",
        "            textposition='auto'\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(barmode='stack', xaxis_title='Final Result', yaxis_title='Count',\n",
        "                      title='Final Results by Highest Education (Stacked)', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 12. Final Results by Module\n",
        "    st.subheader('Final Results by Module')\n",
        "    final_result_module_counts = info_stu.groupby(['final_result', 'code_module']).size().reset_index(name='count')\n",
        "    sorted_results = final_result_module_counts.groupby('final_result').sum().sort_values(by='count', ascending=False).index\n",
        "    pivot_data = final_result_module_counts.pivot(index='final_result', columns='code_module', values='count').reindex(sorted_results).fillna(0)\n",
        "\n",
        "    fig = go.Figure()\n",
        "    for module in pivot_data.columns:\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=pivot_data.index,\n",
        "            y=pivot_data[module],\n",
        "            name=module,\n",
        "            hovertemplate='%{y}',\n",
        "            text=pivot_data[module],\n",
        "            textposition='auto'\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(barmode='stack', xaxis_title='Final Result', yaxis_title='Count',\n",
        "                      title='Final Results by Module (Stacked)', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # 13. Final Results by Module Grouped by Module\n",
        "    st.subheader('Final Results by Module Grouped by Module')\n",
        "    modules = info_stu['code_module'].unique()\n",
        "    for mname in modules:\n",
        "        data = info_stu[info_stu['code_module'] == mname]\n",
        "        final_result_education_counts = data.groupby(['final_result', 'highest_education']).size().reset_index(name='count')\n",
        "        sorted_results = final_result_education_counts.groupby('final_result').sum().index\n",
        "        pivot_data = final_result_education_counts.pivot(index='final_result', columns='highest_education', values='count').reindex(sorted_results).fillna(0)\n",
        "\n",
        "        fig = go.Figure()\n",
        "        for education in pivot_data.columns:\n",
        "            fig.add_trace(go.Bar(\n",
        "                x=pivot_data.index,\n",
        "                y=pivot_data[education],\n",
        "                name=education,\n",
        "                hovertemplate='%{y}',\n",
        "                text=pivot_data[education],\n",
        "                textposition='auto'\n",
        "            ))\n",
        "\n",
        "        fig.update_layout(barmode='stack', xaxis_title='Final Result', yaxis_title='Count',\n",
        "                          title=f'Final Results by Highest Education (Stacked) - Module {mname}', template='plotly_white')\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "    # 14. Correlation Matrix\n",
        "    st.subheader('Correlation Matrix')\n",
        "    student_info = info_stu.copy()\n",
        "\n",
        "    # Preprocess the data\n",
        "    student_info['gender'] = student_info['gender'].map({'M': 0, 'F': 1})\n",
        "    student_info['region'] = student_info['region'].astype('category').cat.codes\n",
        "    student_info['highest_education'] = student_info['highest_education'].astype('category').cat.codes\n",
        "    student_info['imd_band'] = student_info['imd_band'].astype('category').cat.codes\n",
        "    student_info['age_band'] = student_info['age_band'].astype('category').cat.codes\n",
        "    student_info['disability'] = student_info['disability'].map({'N': 0, 'Y': 1})\n",
        "    student_info['code_module'] = student_info['code_module'].astype('category').cat.codes\n",
        "    student_info['code_presentation'] = student_info['code_presentation'].astype('category').cat.codes\n",
        "    final_result_map = {'Distinction': 4, 'Pass': 3, 'Fail': 2, 'Withdrawn': 1}\n",
        "    student_info['final_result'] = student_info['final_result'].map(final_result_map)\n",
        "\n",
        "    correlation_matrix = student_info.corr()\n",
        "\n",
        "    fig = px.imshow(correlation_matrix, text_auto=True, aspect='auto', color_continuous_scale=px.colors.sequential.Blues)\n",
        "    fig.update_layout(title='Correlation Matrix with Final Result as Target', template='plotly_white')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "\n",
        "def as_stu_eda(as_stu):\n",
        "    # Display the head of student information data\n",
        "    st.subheader('Head of Student Information Data')\n",
        "    st.write(as_stu.head(10))\n",
        "\n",
        "    # Score Trend Over Time\n",
        "    st.subheader('Score Trend Over Time')\n",
        "\n",
        "    # Calculate average score and categorize scores\n",
        "    average_score = as_stu.groupby('date_submitted')['score'].mean().reset_index()\n",
        "    average_score['Score_Category'] = pd.cut(average_score['score'], bins=[-float('inf'), 40, float('inf')],\n",
        "                                             labels=['Fail (< 40)', 'Pass (>= 40)'])\n",
        "\n",
        "    # Create an interactive line plot with Plotly\n",
        "    fig = px.line(average_score, x='date_submitted', y='score', color='Score_Category', markers=True,\n",
        "                  title='Score Trend Over Time',\n",
        "                  labels={'date_submitted': 'Date Submitted', 'score': 'Average Score'},\n",
        "                  color_discrete_sequence=px.colors.qualitative.Set2)\n",
        "\n",
        "    # Update layout for better appearance\n",
        "    fig.update_layout(xaxis_title='Date', yaxis_title='Score', template='plotly_white')\n",
        "    fig.update_xaxes(tickangle=45)\n",
        "    fig.update_layout(legend_title_text='Score Category')\n",
        "\n",
        "    # Add markers and lines\n",
        "    fig.add_scatter(x=average_score['date_submitted'], y=average_score['score'],\n",
        "                    mode='markers+lines', marker=dict(size=10, symbol='circle'))\n",
        "\n",
        "    # Add hover information\n",
        "    fig.update_traces(hovertemplate='Date: %{x}<br>Average Score: %{y}')\n",
        "\n",
        "    # Display the Plotly chart in Streamlit\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "\n",
        "def data_merge(as_stu, info_stu):\n",
        "    # Clean info_stu dataset\n",
        "    info_stu_cleaned = info_stu.copy()\n",
        "    info_stu_cleaned['imd_band'].fillna('unknown', inplace=True)  # Fill missing values in 'imd_band'\n",
        "\n",
        "    # Clean as_stu dataset\n",
        "    as_stu_cleaned = as_stu.copy()\n",
        "    as_stu_cleaned.dropna(subset=['score'], inplace=True)  # Drop rows where 'score' is NaN\n",
        "\n",
        "    # Convert date_submitted to datetime format (assuming start_date is defined)\n",
        "    start_date = pd.to_datetime('2023-01-01')  # Replace with your actual start date\n",
        "    as_stu_cleaned['date_submitted'] = start_date + pd.to_timedelta(as_stu_cleaned['date_submitted'] - 1, unit='D')\n",
        "\n",
        "    # Merge datasets\n",
        "    data = pd.merge(as_stu_cleaned, info_stu_cleaned, on='id_student', how='left')\n",
        "\n",
        "    # Fill missing values in 'score' with 0\n",
        "    data['score'].fillna(0, inplace=True)\n",
        "\n",
        "    # Convert categorical columns to numerical\n",
        "    data['gender'] = data['gender'].map({'M': 0, 'F': 1})\n",
        "    data['region'] = data['region'].astype('category').cat.codes\n",
        "    data['highest_education'] = data['highest_education'].astype('category').cat.codes\n",
        "    data['imd_band'] = data['imd_band'].astype('category').cat.codes\n",
        "    data['age_band'] = data['age_band'].astype('category').cat.codes\n",
        "    data['disability'] = data['disability'].map({'N': 0, 'Y': 1})\n",
        "    data['code_module'] = data['code_module'].astype('category').cat.codes\n",
        "    data['code_presentation'] = data['code_presentation'].astype('category').cat.codes\n",
        "\n",
        "    # Map final_result to numerical values\n",
        "    final_result_map = {\n",
        "        'Distinction': 4,\n",
        "        'Pass': 3,\n",
        "        'Fail': 2,\n",
        "        'Withdrawn': 1\n",
        "    }\n",
        "    data['final_result'] = data['final_result'].map(final_result_map)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def decision_tree(student_info_assessments):\n",
        "\n",
        "    st.subheader('Decision Tree Regressor')\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    my_dt = DecisionTreeRegressor(\n",
        "        max_depth=10,                 Limits depth of the tree\n",
        "        min_samples_split=15,        Requires at least 15 samples to consider a split\n",
        "        min_samples_leaf=5,         Requires at least 5 samples per leaf\n",
        "        max_leaf_nodes=200,          Maximum number of leaf nodes\n",
        "    )\n",
        "    my_dt.fit(X_train, y_train)  # Fit to training data\n",
        "    \"\"\")\n",
        "\n",
        "    # Drop rows with any missing values in the features and target\n",
        "    data = student_info_assessments.dropna(subset=['score'])\n",
        "    X = data.drop(columns=['id_student', 'score']).select_dtypes(include=[np.number])\n",
        "    y = data['score']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize the DecisionTreeRegressor with specific parameters to regularize the tree\n",
        "    my_dt = DecisionTreeRegressor(\n",
        "        max_depth=10,                # Limits depth of the tree\n",
        "        min_samples_split=15,       # Requires at least 20 samples to consider a split\n",
        "        min_samples_leaf=5,        # Requires at least 10 samples per leaf\n",
        "        max_leaf_nodes=200,         # Maximum number of leaf nodes\n",
        "    )\n",
        "    my_dt.fit(X_train, y_train)  # Fit to training data\n",
        "\n",
        "    # Predict and calculate R2 score\n",
        "    y_pred = my_dt.predict(X_test)\n",
        "\n",
        "    st.write(\"MSE: \", mean_squared_error(y_test, y_pred))\n",
        "    st.write(\"MAE: \", mean_absolute_error(y_test, y_pred))\n",
        "    st.write(\"R2 Score: \", r2_score(y_test, y_pred))\n",
        "\n",
        "    # Plot the decision tree\n",
        "    plt.figure(figsize=(20, 10))  # Set the size of the plot according to your preference\n",
        "    plot_tree(my_dt, feature_names=X.columns, filled=True)\n",
        "    plt.title('Decision Tree Visualization')\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "    # Plot actual vs predicted values\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(y_test.values, label='Actual Values', marker='o')\n",
        "    plt.plot(y_pred, label='Predicted Values', marker='x')\n",
        "    plt.title('Actual vs Predicted Values')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "    st.subheader('Decision Tree Regressor with Pruning')\n",
        "\n",
        "    # Get the cost complexity pruning path\n",
        "    path = my_dt.cost_complexity_pruning_path(X_train, y_train)\n",
        "    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "    st.write('Total Impurity vs effective alpha for training set')\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
        "    plt.xlabel(\"effective alpha\")\n",
        "    plt.ylabel(\"total impurity of leaves\")\n",
        "    plt.title(\"Total Impurity vs effective alpha for training set\")\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "    alpha_selected = ccp_alphas[2]\n",
        "\n",
        "    st.write('Optimal alpha selected is:: ', alpha_selected)\n",
        "\n",
        "    # Re-train the tree with the selected alpha\n",
        "    my_dt_pruned = DecisionTreeRegressor(\n",
        "        random_state=44,\n",
        "        max_depth=10,\n",
        "        min_samples_split=15,\n",
        "        min_samples_leaf=5,\n",
        "        max_leaf_nodes=20,\n",
        "        ccp_alpha=alpha_selected\n",
        "    )\n",
        "    my_dt_pruned.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the pruned tree\n",
        "    y_pred = my_dt_pruned.predict(X_test)\n",
        "    st.write(\"MSE after pruning:\", mean_squared_error(y_test, y_pred))\n",
        "    st.write(\"MAE after pruning:\", mean_absolute_error(y_test, y_pred))\n",
        "    st.write(\"R2 Score: \", r2_score(list(y_test), list(my_dt_pruned.predict(X_test))))\n",
        "\n",
        "    # Plot the decision tree\n",
        "    plt.figure(figsize=(20,10))  # Set the size of the plot according to your preference\n",
        "    plot_tree(my_dt_pruned, feature_names=X.columns, filled=True)\n",
        "    plt.title('Pruned Decision Tree Visualization')\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "    # Plot actual vs predicted values\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(y_test.values, label='Actual Values', marker='o')\n",
        "    plt.plot(y_pred, label='Predicted Values', marker='x')\n",
        "    plt.title('Actual vs Predicted Values')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "\n",
        "def linear_reg(student_info_assessments):\n",
        "\n",
        "  st.subheader('Linear Regression')\n",
        "\n",
        "  st.write(\"\"\"\n",
        "  my_lm = LinearRegression()\n",
        "  \"\"\")\n",
        "\n",
        "\n",
        "  data = student_info_assessments.dropna(subset=['score'])\n",
        "  X = data.drop(columns=['id_student', 'score']).select_dtypes(include=[np.number])\n",
        "  y = data['score']\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  my_lm = LinearRegression()\n",
        "  my_lm.fit(X = X_train, y = y_train)\n",
        "\n",
        "  y_pred = my_lm.predict(X_test)\n",
        "  st.write(\"MSE \", mean_squared_error(y_test, y_pred))\n",
        "  st.write(\"MAE \", mean_absolute_error(y_test, y_pred))\n",
        "  st.write(\"R2_score \", r2_score(y_test, y_pred))\n",
        "\n",
        "  results = pd.DataFrame({'Actual': y_test[1:1000], 'Predicted': y_pred[1:1000]})\n",
        "\n",
        "  # Plot the results\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.scatterplot(x='Actual', y='Predicted', data=results, alpha=0.5, color='green')\n",
        "  sns.lineplot(x='Actual', y='Actual', data=results, color='red')  # Diagonal line\n",
        "\n",
        "  # Add titles and labels\n",
        "  plt.title('Actual vs Predicted Scores')\n",
        "  plt.xlabel('Actual Scores')\n",
        "  plt.ylabel('Predicted Scores')\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "\n",
        "def arima(df):\n",
        "\n",
        "  st.subheader('ARIMA')\n",
        "\n",
        "  st.write(\"\"\"\n",
        "  model = auto_arima(train_target,\n",
        "                      start_p=1, start_q=1,\n",
        "                      test='adf',       # Use ADF test to find optimal 'd'\n",
        "                      max_p=3, max_q=3, # Maximum p and q\n",
        "                      m=1,              # Frequency of series\n",
        "                      d=0,           # Let model determine 'd'\n",
        "                      seasonal=False,   # No seasonality\n",
        "                      start_P=0,\n",
        "                      trace=True,       # Print status\n",
        "                      error_action='ignore',\n",
        "                      suppress_warnings=True,\n",
        "                      stepwise=True)    # Apply stepwise algorithm\n",
        "  \"\"\")\n",
        "\n",
        "\n",
        "  data = df[['date_submitted','score']].copy()\n",
        "\n",
        "  # Set date_submitted as index]\n",
        "  data = data.set_index('date_submitted')\n",
        "\n",
        "  st.subheader('Testing for Stationarity')\n",
        "\n",
        "  for col in list(data):\n",
        "    d = nsdiffs(data[col],\n",
        "            m=10,\n",
        "            max_D=12,\n",
        "            test='ch')\n",
        "\n",
        "    st.write('Columns:: ', col, ' || d:: ', d)\n",
        "\n",
        "  plot_acf(data['score'], lags=40)\n",
        "  plot_pacf(data['score'], lags=40)\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "  data_daily = data.resample('W').mean()\n",
        "  data_daily = data_daily.sort_index()\n",
        "  data_daily = data_daily.interpolate(method='time')\n",
        "\n",
        "  plt.plot(data_daily['score'])\n",
        "  plt.title(f'Trend for score ')\n",
        "  plt.xlabel('Date Submitted')\n",
        "  plt.ylabel('Score')\n",
        "  plt.show()\n",
        "\n",
        "  adf_test = adfuller(data_daily['score'])\n",
        "  # Output the results\n",
        "  st.write('ADF Statistic: %f' % adf_test[0])\n",
        "  st.write('p-value: %f' % adf_test[1])\n",
        "  st.write('Since p-value < 0.05, stationarity doesnt exist (d = 0)')\n",
        "\n",
        "  train_size = int(len(data_daily) * 0.8)\n",
        "  train_data, test_data = data_daily.iloc[:train_size], data_daily.iloc[train_size:]\n",
        "\n",
        "  # Target variable\n",
        "  train_target = train_data['score']\n",
        "  test_target = test_data['score']\n",
        "\n",
        "  #Running Auto ARIMA\n",
        "  model = auto_arima(train_target,\n",
        "                        start_p=1, start_q=1,\n",
        "                        test='adf',       # Use ADF test to find optimal 'd'\n",
        "                        max_p=1, max_q=1, # Maximum p and q\n",
        "                        m=1,              # Frequency of series\n",
        "                        d=0,           # Let model determine 'd'\n",
        "                        seasonal=False,   # No seasonality\n",
        "                        start_P=0,\n",
        "                        trace=True,       # Print status\n",
        "                        error_action='ignore',\n",
        "                        suppress_warnings=True,\n",
        "                        stepwise=True)    # Apply stepwise algorithm\n",
        "\n",
        "  st.subheader('Arima Model Summary')\n",
        "  st.write(model.summary())\n",
        "\n",
        "  st.subheader('Forecasting')\n",
        "\n",
        "  #forecasting\n",
        "  n_periods = len(test_target)\n",
        "  forecast, conf_int = model.predict(n_periods=n_periods, return_conf_int=True)\n",
        "\n",
        "  # Convert predictions to a DataFrame\n",
        "  forecast_df = pd.DataFrame(forecast, index=test_target.index, columns=['Forecast'])\n",
        "  conf_int_df = pd.DataFrame(conf_int, index=test_target.index, columns=['Lower CI', 'Upper CI'])\n",
        "\n",
        "  mae = mean_absolute_error(test_target, forecast)\n",
        "  rmse = np.sqrt(mean_squared_error(test_target, forecast))\n",
        "  st.write(f'MAE: {mae}')\n",
        "  st.write(f'RMSE: {rmse}')\n",
        "\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.plot(train_target, label='Train')\n",
        "  plt.plot(test_target, label='Test')\n",
        "  plt.plot(forecast_df, label='Forecast')\n",
        "  plt.fill_between(conf_int_df.index,\n",
        "                  conf_int_df['Lower CI'],\n",
        "                  conf_int_df['Upper CI'],\n",
        "                  color='k', alpha=.15)\n",
        "  plt.legend()\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "def merge_data_analysis():\n",
        "  df_assessments_full = pd.merge(as_stu, assesment, on='id_assessment', how='left')\n",
        "  df_student_course = pd.merge(info_stu, course, on=['code_module', 'code_presentation'], how='left')\n",
        "  df_student_full = pd.merge(df_student_course, reg_stu, on=['code_module', 'code_presentation', 'id_student'], how='left')\n",
        "  df_student_full = pd.merge(df_student_full, vle_stu, on=['code_module', 'code_presentation', 'id_student'], how='left')\n",
        "\n",
        "  return df_student_full, df_assessments_full\n",
        "\n",
        "def dropout_analysis():\n",
        "  # Merge dataframes\n",
        "  df_student_full, df_assessments_full = merge_data_analysis()\n",
        "\n",
        "  # Handle missing values\n",
        "  df_student_full.fillna(0, inplace=True)\n",
        "\n",
        "  # Convert categorical columns to strings\n",
        "  categorical_columns = ['gender', 'region', 'highest_education', 'imd_band', 'age_band', 'num_of_prev_attempts', 'final_result']\n",
        "  for column in categorical_columns:\n",
        "      df_student_full[column] = df_student_full[column].astype(str)\n",
        "\n",
        "  # Encode categorical variables\n",
        "  label_encoder = LabelEncoder()\n",
        "  for column in categorical_columns:\n",
        "      df_student_full[column] = label_encoder.fit_transform(df_student_full[column])\n",
        "\n",
        "  # Impute missing values\n",
        "  # Define imputers for different types of columns\n",
        "  num_imputer = SimpleImputer(strategy='mean')\n",
        "  cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "  # Identify numerical and categorical features\n",
        "  num_features = df_student_full.select_dtypes(include=['int64', 'float64']).columns\n",
        "  cat_features = df_student_full.select_dtypes(include=['object']).columns\n",
        "\n",
        "  # Apply imputers\n",
        "  df_student_full[num_features] = num_imputer.fit_transform(df_student_full[num_features])\n",
        "  df_student_full[cat_features] = cat_imputer.fit_transform(df_student_full[cat_features])\n",
        "\n",
        "  # Feature Engineering\n",
        "  df_student_full['total_assessment_score'] = df_assessments_full.groupby('id_student')['score'].transform('sum')\n",
        "  df_student_full['interaction_per_module'] = df_student_full.groupby(['code_module', 'code_presentation'])['sum_click'].transform('mean')\n",
        "\n",
        "  # Define dropout based on unregistration date (1 if date_unregistration is not NaN, else 0)\n",
        "  df_student_full['dropout'] = df_student_full['date_unregistration'].apply(lambda x: 1 if not pd.isna(x) else 0)\n",
        "\n",
        "  # Part 2: Predicting Student Dropout\n",
        "  dropout_features = [\n",
        "      'gender', 'region', 'highest_education', 'imd_band', 'age_band', 'num_of_prev_attempts',\n",
        "      'studied_credits', 'sum_click', 'total_assessment_score'\n",
        "  ]\n",
        "\n",
        "  df_student_full.fillna(0, inplace=True)\n",
        "\n",
        "  X_dropout = df_student_full[dropout_features]\n",
        "  y_dropout = df_student_full['dropout']\n",
        "\n",
        "  # Standardize features\n",
        "  scaler = StandardScaler()\n",
        "  X_dropout_scaled = scaler.fit_transform(X_dropout)\n",
        "\n",
        "  # Train-test split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_dropout_scaled, y_dropout, test_size=0.3, random_state=42)\n",
        "\n",
        "  # Train a RandomForestClassifier\n",
        "  model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Predict\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Evaluate\n",
        "  st.write(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
        "  st.write(classification_report(y_test, y_pred))\n",
        "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "  sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "  plt.title('Confusion Matrix for Dropout Prediction')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('Actual')\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "\n",
        "# -------------------------------------Main Code--------------------------------------------\n",
        "# -------------------------------------Main Code--------------------------------------------\n",
        "# -------------------------------------Main Code--------------------------------------------\n",
        "# -------------------------------------Main Code--------------------------------------------\n",
        "\n",
        "\n",
        "# Title\n",
        "st.title('Open University Learning Analytics Dataset (OULAD) Analysis')\n",
        "\n",
        "# Subheading\n",
        "st.markdown(\"### Dataset Visualization and Analysis\")\n",
        "\n",
        "# Sidebar for navigation\n",
        "st.sidebar.header(\"Contents\")\n",
        "sections = [\n",
        "    \"Introduction\", \"Data Overview\", \"Missing Value Analysis\",\"Exploratory Data Analysis (EDA)\", \"Machine Learning (ML)\"\n",
        "]\n",
        "\n",
        "start_date = pd.Timestamp('2000-01-01')\n",
        "\n",
        "choice = st.sidebar.radio(\"Select a section:\", sections)\n",
        "\n",
        "# Introduction to Streamlit\n",
        "if choice == \"Introduction\":\n",
        "\n",
        "    st.header(\"Introduction to Dataset\")\n",
        "    st.markdown(\"\"\"\n",
        "    This dataset belongs to Open University Online Learning Platform (Also called as \"Virtual Learning Environment(VLE)\") that off-campus students use for accessing the course content, forum discussions, sending assessments and checking out assignment marks etc. It consists of 7 selected courses (mentioned as modules in the dataset). Different presentations indicated with letters \"B\" and \"J\" after year for semester 2 and semester 1 respectively.\n",
        "\n",
        "    Additionally, the dataset includes student demographics such as location, age group, disability, education level, gender etc.\n",
        "    Student assessment marks, interactions with the Virtual Learning Environment (VLE) are also included.\n",
        "\n",
        "    It contains data about courses, students and their interactions with Virtual Learning Environment (VLE) for seven selected courses (called modules).\n",
        "    Presentations of courses start in February and October - they are marked by â€œBâ€ and â€œJâ€ respectively. The dataset consists of tables connected using unique identifiers.\n",
        "    \"\"\")\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------\n",
        "elif choice == \"Data Overview\":\n",
        "    st.header(\"Data Overview\")\n",
        "\n",
        "    datasets = {\n",
        "        \"VLE\": vle,\n",
        "        \"Student Interactions\": vle_stu,\n",
        "        \"Student Registration\": reg_stu,\n",
        "        \"Student Info\": info_stu,\n",
        "        \"Student Assessments\": as_stu,\n",
        "        \"Courses\": course,\n",
        "        \"Assessments\": assesment\n",
        "    }\n",
        "\n",
        "    dataset_choice = st.selectbox(\"Select Dataset for Overview:\", list(datasets.keys()))\n",
        "\n",
        "    #if dataset_choice:\n",
        "    if dataset_choice == 'VLE':\n",
        "        st.subheader(f\"Overview for {dataset_choice}\")\n",
        "        # Column descriptions\n",
        "        st.markdown(\"\"\"\n",
        "         1. **id_site**: Identification number of the material.\n",
        "         2. **code_module**: Identification code for the module.\n",
        "         3. **code_presentation**: Identification code of the presentation.\n",
        "         4. **activity_type**: Role associated with the module material.\n",
        "         5. **week_from**: Week from which the material is planned to be used.\n",
        "         6. **week_to**: Week until which the material is planned to be used.\n",
        "         \"\"\")\n",
        "        st.write(datasets[dataset_choice][:1000])\n",
        "    elif dataset_choice == 'Student Interactions':\n",
        "        st.subheader(f\"Overview for {dataset_choice}\")\n",
        "        # Column descriptions\n",
        "        st.markdown(\"\"\"\n",
        "        1. **code_module**: Identification code for a module.\n",
        "        2. **code_presentation**: Identification code of the module presentation.\n",
        "        3. **id_student**: Unique identification number for the student.\n",
        "        4. **id_site**: Identification number for the VLE material.\n",
        "        5. **date**: Date of studentâ€™s interaction with the material, measured as the number of days since the start of the module-presentation.\n",
        "        6. **sum_click**: Number of times a student interacts with the material.\n",
        "        \"\"\")\n",
        "        st.write(datasets[dataset_choice][:1000])\n",
        "\n",
        "    elif dataset_choice == 'Student Registration':\n",
        "        st.subheader(f\"Overview for {dataset_choice}\")\n",
        "        # Column descriptions\n",
        "        st.markdown(\"\"\"\n",
        "        1. **code_module**: Identification code for a module.\n",
        "        2. **code_presentation**: Identification code of the presentation.\n",
        "        3. **id_student**: Unique identification number for the student.\n",
        "        4. **date_registration**: Date of studentâ€™s registration on the module presentation, measured as the number of days relative to the start of the module-presentation. Negative values indicate registration before the start.\n",
        "        5. **date_unregistration**: Date of studentâ€™s un-registration from the module presentation, measured as the number of days relative to the start of the module-presentation.\n",
        "        \"\"\")\n",
        "        st.write(datasets[dataset_choice][:1000])\n",
        "    elif dataset_choice == 'Student Info':\n",
        "        st.subheader(f\"Overview for {dataset_choice}\")\n",
        "        # Column descriptions\n",
        "        st.markdown(\"\"\"\n",
        "      1. **code_module**: Identification code for the module.\n",
        "      2. **code_presentation**: Identification code of the presentation.\n",
        "      3. **id_student**: Unique identification number for the student.\n",
        "      4. **gender**: Studentâ€™s gender.\n",
        "      5. **region**: Geographic region where the student lived during the module presentation.\n",
        "      6. **highest_education**: Highest education level of the student upon entry to the module presentation.\n",
        "      7. **imd_band**: Index of Multiple Deprivation band of the place where the student lived during the module presentation.\n",
        "      8. **age_band**: Age band of the student.\n",
        "      9. **num_of_prev_attempts**: Number of times the student has attempted this module.\n",
        "      10. **studied_credits**: Total number of credits for the modules the student is currently studying.\n",
        "      11. **disability**: Indicates whether the student has declared a disability.\n",
        "      12. **final_result**: Studentâ€™s final result in the module presentation.\n",
        "      \"\"\")\n",
        "        st.write(datasets[dataset_choice][:1000])\n",
        "    elif dataset_choice == 'Student Assessments':\n",
        "        st.subheader(f\"Overview for {dataset_choice}\")\n",
        "        # Column descriptions\n",
        "        st.markdown(\"\"\"\n",
        "        1. **id_assessment**: Identification number of the assessment.\n",
        "        2. **id_student**: Unique identification number for the student.\n",
        "        3. **date_submitted**: Date of student submission, measured as the number of days since the start of the module presentation.\n",
        "        4. **is_banked**: Status flag indicating whether the assessment result has been transferred from a previous presentation.\n",
        "        5. **score**: Studentâ€™s score in this assessment. The range is from 0 to 100. Scores lower than 40 are interpreted as Fail.\n",
        "        \"\"\")\n",
        "\n",
        "        st.write(datasets[dataset_choice][:1000])\n",
        "    elif dataset_choice == 'Courses':\n",
        "        st.subheader(f\"Overview for {dataset_choice}\")\n",
        "        # Column descriptions\n",
        "        st.markdown(\"\"\"\n",
        "        1. **code_module**: Code name of the module, which serves as the identifier.\n",
        "        2. **code_presentation**: Code name of the presentation. It consists of the year and \"B\" for February start or \"J\" for October start.\n",
        "        3. **length**: Length of the module-presentation in days.\n",
        "        \"\"\")\n",
        "        st.write(datasets[dataset_choice][:1000])\n",
        "    elif dataset_choice == 'Assessments':\n",
        "\n",
        "        st.subheader(f\"Overview for {dataset_choice}\")\n",
        "        # Column descriptions\n",
        "        st.markdown(\"\"\"\n",
        "        1. **code_module**: Identification code of the module to which the assessment belongs.\n",
        "        2. **code_presentation**: Identification code of the presentation to which the assessment belongs.\n",
        "        3. **id_assessment**: Identification number of the assessment.\n",
        "        4. **assessment_type**: Type of assessment. Three types exist: Tutor Marked Assessment (TMA), Computer Marked Assessment (CMA), and Final Exam (Exam).\n",
        "        5. **date**: Final submission date of the assessment, measured as the number of days since the start of the module-presentation.\n",
        "        6. **weight**: Weight of the assessment in %. Typically, Exams are treated separately and have the weight 100%; the sum of all other assessments is 100%. If the information about the final exam date is missing, it is at the end of the last presentation week.\n",
        "        \"\"\")\n",
        "        st.write(datasets[dataset_choice][:1000])\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "elif choice == \"Missing Value Analysis\":\n",
        "    st.header(\"Missing Value Treatment\")\n",
        "\n",
        "    datasets = {\n",
        "        \"VLE\": vle,\n",
        "        \"Student Interactions\": vle_stu,\n",
        "        \"Student Registration\": reg_stu,\n",
        "        \"Student Info\": info_stu,\n",
        "        \"Student Assessments\": as_stu,\n",
        "        \"Courses\": course,\n",
        "        \"Assessments\": assesment\n",
        "    }\n",
        "\n",
        "    dataset_choice = st.selectbox(\"Select Dataset for Missing Value Analysis:\", list(datasets.keys()))\n",
        "\n",
        "    if dataset_choice == 'VLE':\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.subheader(f\"Missing Value Treatment for {dataset_choice}\")\n",
        "      st.write(\"Since the 'Week From' and 'Week to' has around 82% missing values, it is best to drop these columns.\")\n",
        "      st.write (\"vle.drop(columns=['week_from','week_to'],inplace=True)\")\n",
        "\n",
        "    elif dataset_choice == 'Student Interactions':\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.write('No Missing Values')\n",
        "      st.subheader('Transforming date to datetime')\n",
        "      st.write(\"\"\"\n",
        "      - Define the start date\n",
        "      start_date = pd.Timestamp('2000-01-01')\n",
        "\n",
        "      - Convert 'date' to datetime\n",
        "      vle_stu['date'] = vle_stu['date'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "      \"\"\")\n",
        "\n",
        "    elif dataset_choice == 'Student Registration':\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.subheader(f\"Missing Value Treatment for {dataset_choice}\")\n",
        "      st.write(\"Since the 'date_unregistration' has around 65% missing values, it is best to drop these columns.\")\n",
        "\n",
        "      st.subheader('Transforming date_registration to datetime')\n",
        "      st.write(\"\"\"\n",
        "      Define the start date\n",
        "      start_date = pd.Timestamp('2000-01-01')\n",
        "\n",
        "      Convert 'date_submitted' to datetime\n",
        "      reg_stu['date_registration'] = reg_stu['date_registration'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "      \"\"\")\n",
        "\n",
        "    elif dataset_choice == 'Student Info':\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.subheader(f\"Missing Value Treatment for {dataset_choice}\")\n",
        "      st.write(\"Since 'imd_band' has missing value it's better to impute it with dummy value\")\n",
        "      st.write(\"info_stu['imd_band'].fillna('unknown', inplace=True)  # Filling missing values with 'unknown'\")\n",
        "\n",
        "    elif dataset_choice == 'Student Assessments':\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.subheader(f\"Missing Value Treatment for {dataset_choice}\")\n",
        "      st.write(\"Score has missing values. Imputing the missing value with the mean of the score achieved that particular student\")\n",
        "      st.write(\"\"\"\n",
        "      student_means = as_stu.groupby('id_student')['score'].transform('mean')\n",
        "      as_stu['score'].fillna(student_means, inplace=True)\n",
        "      \"\"\")\n",
        "\n",
        "      st.subheader('Transforming date_submitted to datetime')\n",
        "      st.write(\"\"\"\n",
        "      Define the start date\n",
        "      start_date = pd.Timestamp('2000-01-01')\n",
        "\n",
        "      Convert 'date_submitted' to datetime\n",
        "      as_stu['date_submitted'] = as_stu['date_submitted'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "      \"\"\")\n",
        "\n",
        "    elif dataset_choice == 'Courses':\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.write('No Missing Values')\n",
        "\n",
        "    elif dataset_choice == 'Assessments':\n",
        "\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.write(\"'Date' has around 5% missing values so it's better that we drop these missing values as we don't if this exam has been conducted or not\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "elif choice == \"Exploratory Data Analysis (EDA)\":\n",
        "\n",
        "  start_date = pd.Timestamp('2000-01-01')\n",
        "\n",
        "  datasets = [\n",
        "        \"VLE\",\n",
        "        \"Student Interactions\",\n",
        "        \"Student Registration\",\n",
        "        \"Student Info\",\n",
        "        \"Student Assessments\"\n",
        "  ]\n",
        "\n",
        "  dataset_choice = st.selectbox(\"Select Dataset for EDA:\", datasets)\n",
        "\n",
        "  if dataset_choice == 'VLE':\n",
        "    st.subheader(f\"EDA for {dataset_choice}\")\n",
        "\n",
        "    vle_cleaned = vle.copy()\n",
        "    vle_cleaned.drop(columns=['week_from','week_to'],inplace=True)\n",
        "\n",
        "    plotActivityCounts(vle_cleaned)\n",
        "\n",
        "  elif dataset_choice == 'Student Interactions':\n",
        "\n",
        "    vle_stu_cleaned = vle_stu.copy()\n",
        "    # Convert 'date' to datetime\n",
        "    vle_stu_cleaned['date'] = vle_stu_cleaned['date'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "\n",
        "\n",
        "    student_vle_eda(vle_stu_cleaned)\n",
        "\n",
        "  elif dataset_choice == 'Student Registration':\n",
        "\n",
        "    reg_stu_cleaned = reg_stu.copy()\n",
        "    reg_stu_cleaned.drop(columns=['date_unregistration'],inplace=True)\n",
        "    reg_stu_cleaned['date_registration'] = pd.to_numeric(reg_stu_cleaned['date_registration'])\n",
        "    reg_stu_cleaned = reg_stu_cleaned.dropna()\n",
        "    reg_stu_cleaned['date_registration'] = reg_stu_cleaned['date_registration'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "\n",
        "    stu_reg_eda(reg_stu_cleaned)\n",
        "\n",
        "  elif dataset_choice == 'Student Info':\n",
        "    info_stu_cleaned = info_stu.copy()\n",
        "    info_stu_cleaned['imd_band'] = info_stu_cleaned['imd_band'].fillna('unknown', inplace=True)\n",
        "    info_stu_cleaned['num_of_prev_attempts'] = info_stu_cleaned['num_of_prev_attempts'].fillna(info_stu['num_of_prev_attempts'].mean(), inplace=True)\n",
        "\n",
        "    info_stu_eda(info_stu_cleaned)\n",
        "\n",
        "  elif dataset_choice == 'Student Assessments':\n",
        "\n",
        "    as_stu_cleaned = as_stu.copy()\n",
        "    student_means = as_stu_cleaned.groupby('id_student')['score'].transform('mean')\n",
        "    as_stu_cleaned['score'] = as_stu_cleaned['score'].fillna(student_means, inplace=True)\n",
        "    as_stu_cleaned['date_submitted'] = as_stu_cleaned['date_submitted'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "\n",
        "    as_stu_eda(as_stu_cleaned)\n",
        "\n",
        "elif choice == \"Machine Learning (ML)\":\n",
        "\n",
        "  student_info_assessments = data_merge(as_stu, info_stu)\n",
        "\n",
        "\n",
        "  # Combine the assessments information\n",
        "  student_info_assessments = data_merge(as_stu, info_stu)\n",
        "\n",
        "  models = ['Introduction','Linear Regression', 'Decision Tree Regressor', 'ARIMA', 'Dropout_analysis']\n",
        "\n",
        "  model_choice = st.selectbox(\"Select Model for Prediction:\", models)\n",
        "\n",
        "\n",
        "\n",
        "  if model_choice == 'Introduction':\n",
        "    st.subheader(\"Data Merging and Splitting for Modeling\")\n",
        "\n",
        "    st.write('Data Selected: Student Assessment & Student Information')\n",
        "    st.write('data_merge(as_stu, info_stu)')\n",
        "    st.write(\"Final Dataset Size:: 207319 rows Ã— 15 columns\")\n",
        "    st.write (\"Target Column: score\")\n",
        "    st.write(\"Date column:: date_submitted\")\n",
        "\n",
        "    st.write(student_info_assessments.head(150))\n",
        "\n",
        "    st.write(\"Spliting the data into 80% training set and 20% test set\")\n",
        "    st.write(\"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\")\n",
        "\n",
        "  elif(model_choice == 'Linear Regression'):\n",
        "    linear_reg(student_info_assessments)\n",
        "  elif model_choice == 'Decision Tree Regressor':\n",
        "    decision_tree(student_info_assessments)\n",
        "  elif model_choice == 'ARIMA':\n",
        "    arima(student_info_assessments)\n",
        "  elif model_choice == 'Dropout_analysis':\n",
        "    dropout_analysis()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd0NUONyz32d",
        "outputId": "c5ab9d5c-cbda-4fd2-fc88-d760908d22f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.819s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0XGmsaL4wgB",
        "outputId": "a1724d3a-2be8-4cbf-b212-874a2d0a5e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.139.254.253\n"
          ]
        }
      ],
      "source": [
        "!streamlit run /content/app.py --server.port 8055 &>/content/logs.txt & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCst6U2W4wil",
        "outputId": "fa908a5b-fed2-4e8f-ecda-9a05e559dc18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.318s\n",
            "your url is: https://red-lamps-type.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8055\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Hi-JOklX4wl9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}